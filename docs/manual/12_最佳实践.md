# 最佳实践

本章节提供使用 mlp-train 的最佳实践和建议，帮助您更高效地使用该包。

## 训练策略

### 1. 选择合适的模型

根据系统大小和精度要求选择模型：

- **小系统（< 20 原子）**：GAP 或 ACE
- **中等系统（20-100 原子）**：ACE 或 MACE
- **大系统（> 100 原子）**：MACE（支持 GPU 加速）

### 2. 温度选择

- **稳定结构**：300-500 K
- **反应路径探索**：1000-2000 K
- **过渡态**：较低温度（100-300 K）

### 3. 选择方法

- **简单系统**：使用 `AbsDiffE`（默认）
- **复杂系统**：使用 `AtomicEnvSimilarity`
- **异常检测**：使用 `AtomicEnvDistance`

### 4. 迭代策略

```python
# 快速测试
gap.al_train(
    method_name='xtb',      # 使用快速方法
    temp=1000,
    max_active_iters=5,     # 少量迭代
    n_configs_iter=5,      # 少量构型
    max_active_time=500,   # 短时间
)

# 生产运行
gap.al_train(
    method_name='orca',     # 使用精确方法
    temp=1500,
    max_active_iters=50,    # 充分迭代
    n_configs_iter=10,     # 足够构型
    max_active_time=2000,  # 充分探索
)
```

## 性能优化

### 1. 并行设置

```python
# 确保 n_cores 是 n_configs_iter 的倍数
mlt.Config.n_cores = 16
gap.al_train(..., n_configs_iter=8)  # 16 是 8 的倍数

# 根据系统资源调整
# CPU 密集型：n_cores = CPU 核心数
# 内存受限：减少 n_cores
```

### 2. 参考方法选择

- **快速测试**：xTB（秒级）
- **平衡**：ORCA PBE/def2-SVP（分钟级）
- **高精度**：ORCA PBE0/def2-TZVP（小时级）

### 3. GPU 加速

对于 MACE 模型，使用 GPU 可以显著加速：

```python
# 检查 GPU 可用性
import torch
print(torch.cuda.is_available())

# 使用 GPU
mlt.Config.mace_params['device'] = 'cuda'
```

## 数据管理

### 1. 保存训练数据

```python
# 训练后保存
gap.save()

# 手动保存训练数据
gap.training_data.save_xyz('training_data.xyz')
```

### 2. 检查训练质量

```python
# 检查训练数据量
print(f"训练数据点数: {gap.n_train}")

# 检查能量误差
for config in gap.training_data:
    if config.energy.true is not None:
        error = abs(config.energy.delta)
        print(f"能量误差: {error:.4f} eV")
```

### 3. 数据清理

```python
# 移除高能构型
gap.training_data.remove_above_e(max_e_threshold=5.0)

# 移除未计算能量的构型
if gap.training_data.has_a_none_energy:
    gap.training_data.remove_none_energy()
```

## MD 模拟最佳实践

### 1. 时间步长选择

- **小分子**：0.5 fs
- **中等分子**：1.0 fs
- **大分子**：2.0 fs（需谨慎测试）

### 2. 起始构型

```python
# 使用能量最低的构型
config = system.configuration

# 或使用能量最小化后的构型
# （需要额外的能量最小化步骤）
```

### 3. 温度控制

```python
# 逐步升温
trajectory1 = mlt.md.run_mlp_md(..., temp=100)   # 低温平衡
trajectory2 = mlt.md.run_mlp_md(..., temp=300)   # 目标温度
```

### 4. 保存频率

```python
# 短时间模拟：频繁保存
trajectory = mlt.md.run_mlp_md(..., interval=10)

# 长时间模拟：减少保存频率
trajectory = mlt.md.run_mlp_md(..., interval=100)
```

## 高级采样最佳实践

### 1. 元动力学

```python
# 选择合适的 CV
# - CV 应该能够区分不同状态
# - 避免使用快变量

# 自动估计参数
metad.estimate_width(configurations=config_set, mlp=gap)

# 使用良好调温元动力学
metad.bias_factor = 10.0  # 根据系统调整
```

### 2. 伞形采样

```python
# 窗口间距：确保足够重叠
windows = [1.5, 1.8, 2.1, 2.4, 2.7]  # 0.3 间距

# 弹簧常数：足够大但不过大
kappas = [0.15] * len(windows)

# 模拟时间：确保充分采样
umbrella.run(..., fs=10000)
```

## 代码组织

### 1. 模块化

```python
# config.py - 配置管理
import mlptrain as mlt
mlt.Config.n_cores = 16
mlt.Config.orca_keywords = ['PBE', 'def2-SVP', 'EnGrad']

# train.py - 训练脚本
from config import *
# ... 训练代码 ...

# md.py - MD 模拟脚本
from config import *
# ... MD 代码 ...
```

### 2. 错误处理

```python
try:
    gap.al_train(method_name='orca', temp=1000)
except Exception as e:
    print(f"训练失败: {e}")
    # 保存已有数据
    gap.training_data.save_xyz('partial_training_data.xyz')
```

### 3. 日志记录

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
```

## 验证和测试

### 1. 小规模测试

在运行大规模计算前，先进行小规模测试：

```python
# 测试脚本
gap.al_train(
    method_name='xtb',      # 快速方法
    temp=500,
    max_active_iters=2,     # 少量迭代
    n_configs_iter=3,      # 少量构型
    max_active_time=100,   # 短时间
)
```

### 2. 验证训练结果

```python
# 运行短时间 MD 验证
trajectory = mlt.md.run_mlp_md(
    configuration=system.random_configuration(),
    mlp=gap,
    fs=100,         # 短时间
    temp=300,
    dt=0.5,
    interval=10,
)

# 比较预测与真实值
trajectory.compare(gap, 'orca')
```

### 3. 检查能量守恒

```python
# 检查 NVE 模拟的能量守恒
trajectory = mlt.md.run_mlp_md(
    ...,
    temp=0,  # NVE 模拟
)

energies = [config.energy.predicted for config in trajectory]
energy_std = np.std(energies)
print(f"能量标准差: {energy_std:.6f} eV")
```

## 资源管理

### 1. 内存管理

```python
# 定期清理不需要的数据
del old_trajectory
del old_config_set
import gc
gc.collect()
```

### 2. 磁盘空间

```python
# 只保存必要的文件
trajectory.save('important_trajectory.xyz')

# 删除中间文件
import os
os.remove('temp_file.xyz')
```

### 3. 计算资源

```python
# 根据可用资源调整
# - CPU 核心数
# - 内存大小
# - GPU 可用性

mlt.Config.n_cores = min(16, os.cpu_count())
```

## 调试技巧

### 1. 逐步调试

```python
# 1. 检查系统创建
system = mlt.System(mlt.Molecule('water.xyz'), box=None)
print(f"系统原子数: {len(system.molecule.atoms)}")

# 2. 检查 MLP 创建
gap = mlt.potentials.GAP('water', system=system)
print(f"MLP 名称: {gap.name}")

# 3. 检查训练数据
print(f"初始训练数据: {gap.n_train}")

# 4. 逐步训练
gap.al_train(..., max_active_iters=1)  # 先运行一次迭代
print(f"训练后数据: {gap.n_train}")
```

### 2. 可视化

```python
# 绘制能量分布
energies = [c.energy.true for c in gap.training_data if c.energy.true is not None]
import matplotlib.pyplot as plt
plt.hist(energies, bins=50)
plt.xlabel('Energy (eV)')
plt.ylabel('Count')
plt.savefig('energy_distribution.png')
```

## 文档和记录

### 1. 记录参数

```python
# 在脚本中记录重要参数
"""
训练参数：
- 模型：GAP
- 参考方法：ORCA PBE/def2-SVP
- 温度：1000 K
- 迭代次数：50
- 选择方法：AbsDiffE (e_thresh=0.1)
"""
```

### 2. 版本控制

```python
# 记录使用的版本
import mlptrain as mlt
print(f"mlp-train 版本: {mlt.__version__}")
```

## 常见陷阱

### 1. 避免过度训练

- 不要无限制地增加训练数据
- 监控验证误差，避免过拟合

### 2. 避免温度过高

- 过高的温度可能导致不合理的构型
- 根据系统特性选择合适的温度

### 3. 避免参数不当

- 确保所有参数设置合理
- 参考示例和文档

## 总结

遵循这些最佳实践可以帮助您：

1. **提高效率**：优化计算资源使用
2. **保证质量**：确保训练和模拟的准确性
3. **便于维护**：组织良好的代码易于维护和扩展
4. **减少错误**：避免常见陷阱和问题

## 下一步

- 查看 [API 参考](10_API参考.md) 了解详细 API
- 参考 [示例教程](09_示例教程.md) 学习实际应用
- 遇到问题时查看 [常见问题](11_常见问题.md)
